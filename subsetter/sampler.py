import abc
import functools
import json
import logging
import os
import re
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple

import sqlalchemy as sa
from sqlalchemy.dialects import mysql, postgresql, sqlite
from sqlalchemy.ext.compiler import compiles
from sqlalchemy.sql.compiler import SQLCompiler
from sqlalchemy.sql.expression import ClauseElement, Executable

from subsetter.common import DatabaseConfig, parse_table_name
from subsetter.config_model import (
    ConflictStrategy,
    DatabaseOutputConfig,
    DirectoryOutputConfig,
    SamplerConfig,
)
from subsetter.filters import FilterOmit, FilterView, FilterViewChain
from subsetter.metadata import DatabaseMetadata
from subsetter.plan_model import SQLTableIdentifier
from subsetter.planner import SubsetPlan
from subsetter.solver import toposort

try:
    from tqdm import tqdm  # type: ignore
except ImportError:

    def tqdm(x, **_):
        return x


LOGGER = logging.getLogger(__name__)
SOURCE_BUFFER_SIZE = 1024
DESTINATION_BUFFER_SIZE = 1024
SUBSETTER_COMPACT_COLUMN = "_sbsttr_id"


class TempTableCreator:
    """
    Help generate temporary tables. Attempts to give the temporary tables
    friendly names that will also not interfere with existing tables. Avoids
    giving any two tables generated with an instance of this class the same
    name.
    """

    NAME_PREFIX = "_sbsttr_"

    def __init__(self) -> None:
        self.name_counts: Dict[str, int] = {}

    def _generate_name(self, name: str) -> str:
        name = "".join(ch for ch in name if ch.isalnum())[:40]
        if not name:
            name = "anon"
        cnt = self.name_counts.get(name, 0)
        self.name_counts[name] = cnt + 1
        if cnt:
            return f"{self.NAME_PREFIX}{name}_{cnt}"
        return f"{self.NAME_PREFIX}{name}"

    def create(
        self,
        conn,
        schema: str,
        select: sa.Select,
        *,
        name: str = "",
        primary_key: Tuple[str, ...] = (),
    ) -> Tuple[sa.Table, int]:
        """
        Create a temporary table on the passed connection generated by the passed
        Select object. This method will return a

        Parameters
            conn: The connection to create the temporary table within. Temporary tables
                  are private to the connection that created them and are cleaned up
                  after the connection is closed.
            schema: The schema to create the temporary table within. For some dialects
                    temporary tables always exist in their own schema and this parameter
                    will be ignored.
            primary_key: If set will mark the set of columns passed as primary keys in
                         the temporary table. This tuple should match a subset of the
                         column names in the select query.

        Returns a tuple containing the generated table object and the number of rows that
        were inserted in the table.
        """
        dialect = conn.engine.dialect

        # Some dialects can only create temporary tables in an implicit schema
        temp_schema: Optional[str] = schema
        if dialect.name in ("postgresql", "sqlite"):
            temp_schema = None

        temp_name = self._generate_name(name)

        # Create the temporary table from the select statement. Mark the requested
        # columns as part of the primary key.
        metadata = sa.MetaData()
        table_obj = sa.Table(
            temp_name,
            metadata,
            schema=temp_schema,
            prefixes=["TEMPORARY"],
            *(
                sa.Column(col.name, col.type, primary_key=col.name in primary_key)
                for col in select.selected_columns
            ),
        )
        try:
            metadata.create_all(conn)
        except Exception as exc:  # pylint: disable=broad-exception-caught
            # TODO: Is this still needed?
            #
            # Some client/server combinations report a read-only error even though the temporary
            # table creation actually succeeded. We'll just swallow the error here and if there
            # was a real issue it'll get flagged again when we query against it.
            if "--read-only" not in str(exc):
                raise

        # Copy data into the temporary table
        result = conn.execute(
            table_obj.insert().from_select(list(table_obj.columns), select)
        )
        result = conn.execute(table_obj.select())

        return table_obj, result.rowcount


# pylint: disable=too-many-ancestors,abstract-method
class ConflictInsert(Executable, ClauseElement):
    inherit_cache = False
    _inline = False
    _return_defaults = False

    def __init__(
        self,
        conflict_strategy: ConflictStrategy,
        table: sa.Table,
        columns: Iterable[str],
    ) -> None:
        self.conflict_strategy = conflict_strategy
        self.table = table
        self.non_pk_columns = set(columns)

        if not self.table.primary_key:
            # We only attempt to do this if there is a primary key
            self.conflict_strategy = "error"
        else:
            for column in self.table.primary_key:
                self.non_pk_columns.remove(column.name)
            if self.conflict_strategy == "replace" and not self.non_pk_columns:
                # If there are no other columns outside of the primary key the replace
                # strategy is meaningless and we should just skip.
                self.conflict_strategy = "skip"

    def _insert_generic(self, compiler: SQLCompiler, **kwargs) -> str:
        if self.conflict_strategy == "replace":
            raise RuntimeError(
                "'replace' conflict strategy is not supported for this dialect"
            )
        if self.conflict_strategy == "skip":
            raise RuntimeError(
                "'skip' conflict strategy is not supported for this dialect"
            )
        return compiler.process(sa.insert(self.table), **kwargs)

    def _insert_mysql(self, compiler: SQLCompiler, **kwargs) -> str:
        stmt = mysql.insert(self.table)
        if self.conflict_strategy == "replace":
            stmt = stmt.on_duplicate_key_update(
                {col: stmt.inserted[col] for col in self.non_pk_columns}
            )
        if self.conflict_strategy == "skip":
            stmt = stmt.prefix_with("IGNORE")
        return compiler.process(stmt, **kwargs)

    def _insert_postgresql(self, compiler: SQLCompiler, **kwargs) -> str:
        stmt = postgresql.insert(self.table)
        if self.conflict_strategy == "replace":
            stmt = stmt.on_conflict_do_update(
                index_elements=self.table.primary_key,
                set_={col: stmt.excluded[col] for col in self.non_pk_columns},
            )
        if self.conflict_strategy == "skip":
            stmt = stmt.on_conflict_do_nothing()
        return compiler.process(stmt, **kwargs)

    def _insert_sqlite(self, compiler: SQLCompiler, **kwargs) -> str:
        stmt = sqlite.insert(self.table)
        if self.conflict_strategy == "replace":
            stmt = stmt.on_conflict_do_update(
                index_elements=self.table.primary_key,
                set_=stmt.excluded,  # type: ignore
            )
        if self.conflict_strategy == "skip":
            stmt = stmt.on_conflict_do_nothing()
        return compiler.process(stmt, **kwargs)


compiles(ConflictInsert, "sqlite")(ConflictInsert._insert_sqlite)
compiles(ConflictInsert, "postgresql")(ConflictInsert._insert_postgresql)
compiles(ConflictInsert, "mysql")(ConflictInsert._insert_mysql)
compiles(ConflictInsert)(ConflictInsert._insert_generic)


def _autoincrement_needs_updating(engine) -> bool:
    """
    Returns True if the engine doesn't automatically update auto-increment values
    to the highest inserted value for that column. Currently only postgresql
    does not do this among supported engines.
    """
    return engine.dialect.name == "postgresql"


def _autoincrement_get(engine, table: sa.Table) -> Optional[int]:
    """
    Get the current auto-increment state.
    """
    if not _autoincrement_needs_updating(engine) or table.autoincrement_column is None:
        return None
    assert engine.dialect.name == "postgresql"

    try:
        with engine.connect() as conn:
            result = conn.execute(
                sa.text("SELECT currval(pg_get_serial_sequence(:tab, :col))"),
                {
                    "tab": f"{table.schema}.{table.name}",
                    "col": table.autoincrement_column.name,
                },
            )
            row = next(iter(result), None)
            return row[0] if row else None
    except sa.exc.OperationalError as exc:
        if "not yet defined in this session" not in str(exc):
            raise
        return None


def _autoincrement_update(engine, table: sa.Table, value: Optional[int]) -> None:
    """
    Update the auto-increment state to ensure the next generated value is greater
    than value.
    """
    if (
        not _autoincrement_needs_updating(engine)
        or table.autoincrement_column is None
        or value is None
    ):
        return
    assert engine.dialect.name == "postgresql"

    curval = _autoincrement_get(engine, table)
    if curval is not None and curval > value:
        return

    with engine.connect() as conn:
        conn.execute(
            sa.text("SELECT setval(pg_get_serial_sequence(:tab, :col), :val)"),
            {
                "tab": f"{table.schema}.{table.name}",
                "col": table.autoincrement_column.name,
                "val": value,
            },
        )
        conn.commit()


def _multiply_column(
    value: Optional[int], multiplier: int, iteration: int
) -> Optional[int]:
    if value is None:
        return None
    return value * multiplier + iteration


class SamplerOutput(abc.ABC):
    def output_result_set(
        self,
        schema: str,
        table_name: str,
        columns: List[str],
        result_set,
        *,
        filter_view: Optional[FilterView] = None,
        multiplier: int = 1,
        column_multipliers: Optional[Set[str]] = None,
    ) -> None:
        pass

    def truncate(self) -> None:
        """Delete any existing data that could interfere with output destination"""

    def create(self) -> None:
        """Create any missing tables in destination from the source schema"""

    def prepare(self) -> None:
        """Called just prior to sampling after truncate/create have executed"""

    @abc.abstractmethod
    def insert_order(self) -> List[str]:
        """Return the order to insert data that respects foreign key relationships"""

    @staticmethod
    def from_config(
        config: Any, plan: SubsetPlan, source_meta: DatabaseMetadata
    ) -> "SamplerOutput":
        if isinstance(config, DirectoryOutputConfig):
            return DirectoryOutput(config, plan, source_meta)
        if isinstance(config, DatabaseOutputConfig):
            return DatabaseOutput(config, plan, source_meta)
        raise RuntimeError("Unknown config type")


class DirectoryOutput(SamplerOutput):
    def __init__(
        self,
        config: DirectoryOutputConfig,
        plan: SubsetPlan,
        source_meta: DatabaseMetadata,
    ) -> None:
        self.directory = config.directory
        self.plan = plan
        self.source_meta = source_meta

    def insert_order(self) -> List[str]:
        return list(self.plan.queries)

    def output_result_set(
        self,
        schema: str,
        table_name: str,
        columns: List[str],
        result_set,
        *,
        filter_view: Optional[FilterView] = None,
        multiplier: int = 1,
        column_multipliers: Optional[Set[str]] = None,
    ) -> None:
        output_path = os.path.join(self.directory, f"{schema}.{table_name}.json")
        columns_out = filter_view.columns_out if filter_view else columns

        multiplied_indexes = [
            ind
            for ind, col_name in enumerate(columns_out)
            if col_name in (column_multipliers or set())
        ]
        with open(output_path, "w", encoding="utf-8") as fdump:
            for row in result_set:
                for iteration in range(multiplier):
                    out_row = filter_view.filter_view(row) if filter_view else list(row)
                    for index in multiplied_indexes:
                        out_row[index] = _multiply_column(
                            out_row[index], multiplier, iteration
                        )
                    json.dump(dict(zip(columns_out, out_row)), fdump, default=str)
                    fdump.write("\n")


class DatabaseOutput(SamplerOutput):
    # Pylint getting weirdly confused about the self.meta member
    # pylint: disable=no-member

    def __init__(
        self,
        config: DatabaseOutputConfig,
        plan: SubsetPlan,
        source_meta: DatabaseMetadata,
    ) -> None:
        self.engine = config.database_engine(env_prefix="SUBSET_DESTINATION_")
        self.remap = config.remap
        self.conflict_strategy = config.conflict_strategy
        self.merge = config.merge
        self.plan = plan
        self.source_meta = source_meta
        self.table_offsets: Dict[Tuple[str, str], Dict[str, int]] = {}
        self.passthrough_tables = set(self.plan.passthrough)

        self.table_remap: Dict[str, str] = {}
        for table in plan.queries:
            remapped_table = ".".join(self._remap_table(*parse_table_name(table)))
            if self.table_remap.setdefault(remapped_table, table) != table:
                raise ValueError(
                    f"Multiple tables remapped to the same name {remapped_table}"
                )

        self.meta, self.additional_tables = DatabaseMetadata.from_engine(
            self.engine,
            list(self.table_remap),
            close_backward=True,
        )

    def prepare(self) -> None:
        """
        If merge mode is enabled calculate the offsets to assign to each primary
        key for each table. Uses the foreign key analysis done at the source
        database to inform what foreign key relationships are expected to exist
        in the destination.
        """
        if not self.merge:
            return

        pk_max_values = {}
        self.table_offsets.clear()
        for source_table in self.plan.queries:
            if source_table in self.passthrough_tables:
                continue

            schema, table_name = self._remap_table(*parse_table_name(source_table))
            self.table_offsets[(schema, table_name)] = {}
            table = self.meta.tables[(schema, table_name)]
            if len(table.primary_key) != 1:
                LOGGER.warning(
                    "Cannot merge multi-column primary key for table %s.%s, ignoring",
                    schema,
                    table_name,
                )
                continue

            pk_col = table.table_obj.columns[table.primary_key[0]]
            if not issubclass(pk_col.type.python_type, int):  # type: ignore
                LOGGER.warning(
                    "Cannot merge non-integer primary key for table %s.%s, ignoring",
                    schema,
                    table_name,
                )
                continue

            with self.engine.connect() as conn:
                max_pk_val = conn.scalar(sa.select(sa.func.max(pk_col)))

            pk_max_values[source_table] = max_pk_val
            if max_pk_val is not None:
                self.table_offsets[(schema, table_name)][pk_col.name] = max_pk_val + 1

        for source_table in self.plan.queries:
            if source_table in self.passthrough_tables:
                continue

            src_schema, src_table_name = parse_table_name(source_table)
            src_table = self.source_meta.tables[(src_schema, src_table_name)]
            for fk in src_table.foreign_keys:
                if len(fk.columns) != 1:
                    continue
                if (
                    fk.dst_columns
                    != self.source_meta.tables[
                        (fk.dst_schema, fk.dst_table)
                    ].primary_key
                ):
                    continue
                offset = pk_max_values.get(f"{fk.dst_schema}.{fk.dst_table}")
                if offset is not None:
                    self.table_offsets[self._remap_table(src_schema, src_table_name)][
                        fk.columns[0]
                    ] = (offset + 1)

    def create(self) -> None:
        """Create any missing tables in destination from the source schema"""
        source_meta = self.source_meta
        metadata_obj = sa.MetaData()

        table_obj_map = {}
        tables_created = set()
        for remapped_table, table in self.table_remap.items():
            remap_schema, remap_table = parse_table_name(remapped_table)

            if (remap_schema, remap_table) in self.meta.tables:
                table_obj_map[table] = self.meta.tables[
                    (remap_schema, remap_table)
                ].table_obj
                continue

            table_obj = source_meta.tables[parse_table_name(table)].table_obj
            table_obj_map[table] = sa.Table(
                remap_table,
                metadata_obj,
                *(
                    sa.Column(
                        col.name,
                        col.type,
                        nullable=col.nullable,
                        primary_key=col.primary_key,
                    )
                    for col in table_obj.columns
                ),
                schema=remap_schema,
            )
            tables_created.add(table_obj_map[table])

        def _remap_cols(cols: Iterable[sa.Column]) -> List[sa.Column]:
            return [
                table_obj_map[f"{col.table.schema}.{col.table.name}"].columns[col.name]
                for col in cols
            ]

        # Copy table constraints including foreign key constraints.
        for table, remapped_table_obj in table_obj_map.items():
            if remapped_table_obj not in tables_created:
                continue

            table_obj = source_meta.tables[parse_table_name(table)].table_obj
            for constraint in table_obj.constraints:
                if isinstance(constraint, sa.UniqueConstraint):
                    remapped_table_obj.append_constraint(
                        sa.UniqueConstraint(*_remap_cols(constraint.columns))
                    )
                if isinstance(constraint, sa.CheckConstraint):
                    remapped_table_obj.append_constraint(
                        sa.CheckConstraint(constraint.sqltext)
                    )
                if isinstance(constraint, sa.ForeignKeyConstraint):
                    fk_cols = _remap_cols(elem.column for elem in constraint.elements)
                    remapped_table_obj.append_constraint(
                        sa.ForeignKeyConstraint(
                            _remap_cols(constraint.columns),
                            fk_cols,
                            name=constraint.name,
                            use_alter=True,
                        )
                    )

            for index_idx, index in enumerate(table_obj.indexes):
                sa.Index(
                    f"idx_subsetter_{remapped_table_obj.name}_{index_idx}",
                    *(
                        (
                            remapped_table_obj.columns[col.name]
                            if isinstance(col, sa.Column)
                            else col
                        )
                        for col in index.columns
                    ),
                    unique=index.unique,
                    dialect_options=index.dialect_options,
                    **index.dialect_kwargs,
                )

        if tables_created:
            LOGGER.info("Creating %d tables in destination", len(tables_created))
            metadata_obj.create_all(bind=self.engine)
            for remapped_table_obj in tables_created:
                self.meta.track_new_table(remapped_table_obj)

    def truncate(self) -> None:
        for schema, table_name in self.additional_tables:
            LOGGER.info(
                "Found additional table %s.%s to truncate",
                schema,
                table_name,
            )

        with self.engine.connect() as conn:
            for table in reversed(self.meta.toposort()):
                LOGGER.info("Truncating table %s", table)
                conn.execute(sa.delete(table.table_obj))
                conn.commit()

    def _remap_table(self, schema: str, table_name: str) -> Tuple[str, str]:
        table = f"{schema}.{table_name}"
        for remap_pattern in self.remap:
            table = re.sub(remap_pattern.search, remap_pattern.replace, table)
        return parse_table_name(table)

    def insert_order(self) -> List[str]:
        result = []
        insert_order_mapped = [str(table) for table in self.meta.toposort()]
        for table in insert_order_mapped:
            remapped_table = self.table_remap.get(table)
            if remapped_table:
                result.append(remapped_table)
        return result

    def output_result_set(
        self,
        schema: str,
        table_name: str,
        columns: List[str],
        result_set,
        *,
        filter_view: Optional[FilterView] = None,
        multiplier: int = 1,
        column_multipliers: Optional[Set[str]] = None,
    ) -> None:
        (src_schema, src_table_name) = (schema, table_name)
        schema, table_name = self._remap_table(src_schema, src_table_name)
        table = self.meta.tables[(schema, table_name)]

        columns_out = filter_view.columns_out if filter_view else columns
        missing_columns = {
            col for col in columns_out if col not in table.table_obj.columns
        }
        if missing_columns:
            raise ValueError(
                f"Destination table {schema}.{table_name} is missing expected columns {missing_columns}"
            )

        # Automatically omit any included computed columns
        computed_columns = [
            col for col in columns_out if table.table_obj.columns[col].computed
        ]
        if computed_columns:
            omit_filter = FilterOmit(columns_out, computed_columns)
            columns_out = omit_filter.columns_out
            if filter_view:
                filter_view = FilterViewChain(
                    filter_view.columns_in,
                    columns_out,
                    (filter_view, omit_filter),
                )
            else:
                filter_view = omit_filter

        offsets = None
        if table_offsets := self.table_offsets.get((schema, table_name)):
            offsets = [
                (index, table_offsets[col])
                for index, col in enumerate(columns_out)
                if col in table_offsets
            ]

        multiplied_indexes = [
            ind
            for ind, col_name in enumerate(columns_out)
            if col_name in (column_multipliers or set())
        ]

        autoinc_index = -1
        autoinc_max_written = None
        if (
            _autoincrement_needs_updating(self.engine)
            and table.table_obj.autoincrement_column is not None
        ):
            try:
                autoinc_index = columns_out.index(
                    table.table_obj.autoincrement_column.name
                )
            except ValueError:
                pass

        buffer: List[tuple] = []

        conflict_strategy = self.conflict_strategy
        if self.merge and f"{src_schema}.{src_table_name}" in self.passthrough_tables:
            conflict_strategy = "skip"

        def _flush_buffer():
            with self.engine.connect() as conn:
                conn.execute(
                    ConflictInsert(conflict_strategy, table.table_obj, columns_out),
                    [dict(zip(columns_out, row)) for row in buffer],
                )
                conn.commit()
            buffer.clear()

        for row in result_set:
            for iteration in range(multiplier):
                out_row = filter_view.filter_view(row) if filter_view else row
                for index in multiplied_indexes:
                    out_row[index] = _multiply_column(
                        out_row[index], multiplier, iteration
                    )
                if offsets is not None:
                    for index, offset in offsets:
                        if out_row[index] is not None:
                            out_row[index] += offset
                if autoinc_index != -1:
                    if autoinc_max_written is None:
                        autoinc_max_written = out_row[autoinc_index]
                    else:
                        autoinc_max_written = max(
                            autoinc_max_written, out_row[autoinc_index]
                        )
                buffer.append(tuple(out_row))
                if len(buffer) > DESTINATION_BUFFER_SIZE:
                    _flush_buffer()
        if buffer:
            _flush_buffer()

        _autoincrement_update(self.engine, table.table_obj, autoinc_max_written)


class Sampler:
    def __init__(self, source: DatabaseConfig, config: SamplerConfig) -> None:
        self.config = config
        self.source_engine = source.database_engine(env_prefix="SUBSET_SOURCE_")
        self.compact_columns: Dict[Tuple[str, str], Set[str]] = {}
        self.temp_tables = TempTableCreator()
        self.passthrough_tables: Set[str] = set()

    def sample(
        self,
        plan: SubsetPlan,
        *,
        truncate: bool = False,
        create: bool = False,
    ) -> None:
        self.passthrough_tables = set(plan.passthrough)
        meta, _ = DatabaseMetadata.from_engine(self.source_engine, list(plan.queries))
        if self.config.infer_foreign_keys != "none":
            meta.infer_missing_foreign_keys(
                infer_all=self.config.infer_foreign_keys == "all"
            )
        self._validate_filters(meta)

        table_column_multipliers = self._get_multiplied_columns(meta, plan)

        output = SamplerOutput.from_config(self.config.output, plan, meta)
        if create:
            output.create()
        insert_order = output.insert_order()
        if truncate:
            output.truncate()
        output.prepare()

        self.compact_columns = self._get_compact_columns(meta)
        with self.source_engine.execution_options().connect() as conn:
            self._materialize_tables(meta, conn, plan)
            self._copy_results(
                output, conn, meta, plan, insert_order, table_column_multipliers
            )

    def _get_compact_columns(
        self, meta: DatabaseMetadata
    ) -> Dict[Tuple[str, str], Set[str]]:
        """
        Calculate the set of columns that need to be compacted by table.
        """
        compact_columns = {}

        for table, cols in self.config.compact.columns.items():
            if not cols:
                continue
            table_key = parse_table_name(table)
            if table_key not in meta.tables:
                LOGGER.warning(
                    "Table %s has columns configured for compaction but is not found",
                    table,
                )
            elif table in self.passthrough_tables:
                LOGGER.warning(
                    "Cannot compact columns on passthrough table %s",
                    table,
                )
            else:
                compact_columns[table_key] = set(cols)

        if (
            not self.config.compact.primary_keys
            and not self.config.compact.auto_increment_keys
        ):
            return compact_columns

        for table_key, table_meta in meta.tables.items():
            if len(table_meta.primary_key) != 1:
                continue
            if f"{table_key[0]}.{table_key[1]}" in self.passthrough_tables:
                continue

            col = table_meta.table_obj.columns[table_meta.primary_key[0]]
            if not issubclass(col.type.python_type, int):  # type: ignore
                continue

            if (
                self.config.compact.primary_keys
                or table_meta.table_obj.autoincrement_column is not None
            ):
                compact_columns.setdefault(table_key, set()).add(col.name)

        for table_key, table_meta in meta.tables.items():
            table_compact_cols = compact_columns.get(table_key, set())
            for fk in table_meta.foreign_keys:
                for col_name in fk.columns:
                    if col_name in table_compact_cols:
                        raise ValueError(
                            f"Cannot compact column {table_key[0]}.{table_key[1]}.{col_name} within foreign key"
                        )

        return compact_columns

    def _materialization_order(
        self, meta: DatabaseMetadata, plan: SubsetPlan
    ) -> List[Tuple[str, str, int]]:
        """
        Returns a list of tables that need to materialized. This list is a tuple
        of the form (schema, table, max_ref_count). Some dialects (i.e. mysql) require
        making multiple copies of a temp table if they are referenced multiple times.
        """

        def _record_sampled_tables(
            counter: Dict[Tuple[str, str], int], ident: SQLTableIdentifier
        ) -> sa.Table:
            key = (ident.table_schema, ident.table_name)
            if ident.sampled:
                counter[key] = counter.get(key, 0) + 1
            return meta.tables[key].table_obj

        dep_graph: Dict[Tuple[str, str], Set[Tuple[str, str]]] = {}
        max_ref_counts: Dict[Tuple[str, str], int] = {}
        for table, query in plan.queries.items():
            counter: Dict[Tuple[str, str], int] = {}
            query.build(functools.partial(_record_sampled_tables, counter))
            dep_graph[parse_table_name(table)] = set(counter.keys())

            # For calculating max ref count also need to count joins needed at sampling time
            # to facilitate compaction.
            for fk in meta.tables[parse_table_name(table)].foreign_keys:
                key = (fk.dst_schema, fk.dst_table)
                compact_cols = self.compact_columns.get(key, set())
                if any(col in compact_cols for col in fk.dst_columns):
                    counter[key] = counter.get(key, 0) + 1

            for key, count in counter.items():
                max_ref_counts[key] = max(max_ref_counts.get(key, 0), count)

        order: List[Tuple[str, str]] = toposort(dep_graph)
        return [
            (schema, table_name, max_ref_counts[(schema, table_name)])
            for schema, table_name in order
            if (schema, table_name) in max_ref_counts
        ]

    def _materialize_tables(
        self,
        meta: DatabaseMetadata,
        conn: sa.Connection,
        plan: SubsetPlan,
    ) -> None:
        materialization_order = self._materialization_order(meta, plan)
        for schema, table_name, ref_count in materialization_order:
            table = meta.tables[(schema, table_name)]
            query = plan.queries[f"{schema}.{table_name}"]

            table_q = query.build(meta.sql_build_context())
            if (schema, table_name) in self.compact_columns:
                subq = table_q.subquery()
                table_q = sa.select(
                    (
                        sa.func.row_number(type_=sa.Integer).over(
                            order_by=[
                                subq.c[col.name] for col in table.table_obj.primary_key
                            ],
                        )
                        + self.config.compact.start_key
                        - 1
                    ).label(SUBSETTER_COMPACT_COLUMN),
                    subq,
                )

            LOGGER.info(
                "Materializing sample for %s.%s",
                schema,
                table_name,
            )
            meta.temp_tables[(schema, table_name, 0)], rowcount = (
                self.temp_tables.create(
                    conn,
                    schema,
                    table_q,
                    name=table_name,
                    primary_key=table.primary_key,
                )
            )
            LOGGER.info(
                "Materialized %d rows for %s.%s in temporary table",
                rowcount,
                schema,
                table_name,
            )

            if meta.supports_temp_reopen:
                continue

            # Create additional copies of the temporary table if needed. This is
            # to work around an issue on mysql with reopening temporary tables.
            for index in range(1, ref_count):
                meta.temp_tables[(schema, table_name, index)], _ = (
                    self.temp_tables.create(
                        conn,
                        schema,
                        meta.temp_tables[(schema, table_name, 0)].select(),
                        name=table_name,
                        primary_key=table.primary_key,
                    )
                )
                LOGGER.info(
                    "Copied materialization of %s.%s",
                    schema,
                    table_name,
                )

    def _copy_results(
        self,
        output: SamplerOutput,
        conn,
        meta: DatabaseMetadata,
        plan: SubsetPlan,
        insert_order: List[str],
        table_column_multipliers: Dict[str, Set[str]],
    ):
        for table in tqdm(insert_order, desc="table progress", unit="tables"):
            schema, table_name = parse_table_name(table)

            query = plan.queries.get(table)
            if query is None:
                continue

            LOGGER.info("Sampling %s.%s ...", schema, table_name)

            build_ctx = meta.sql_build_context()
            if (schema, table_name, 0) in meta.temp_tables:
                query_stmt = meta.temp_tables[(schema, table_name, 0)].select()
            else:
                query_stmt = query.build(build_ctx)

            # Figure out what columns are foreign keys to compacted columns.
            # We must update the value for that column in this table to match
            # the new compacted column.
            src_table = meta.tables[(schema, table_name)]
            remote_compact_cols = {}
            for fk in src_table.foreign_keys:
                for src_col, dst_col in zip(fk.columns, fk.dst_columns):
                    if dst_col in self.compact_columns.get(
                        (fk.dst_schema, fk.dst_table), set()
                    ):
                        remote_compact_cols[src_col] = (
                            fk.dst_schema,
                            fk.dst_table,
                            dst_col,
                        )

            # Update the query to reflect compaction if needed.
            compact_cols = self.compact_columns.get((schema, table_name), set())
            if remote_compact_cols or compact_cols:
                subq = query_stmt.subquery()
                id_col = subq.columns.get(
                    SUBSETTER_COMPACT_COLUMN,
                    sa.func.row_number(type_=sa.Integer).over(
                        order_by=[
                            subq.c[col.name] for col in src_table.table_obj.primary_key
                        ],
                    )
                    + self.config.compact.start_key
                    - 1,
                )

                # Determine the final values for each column in the sampled table.
                # Each column either comes directly from the source table, is locally
                # compacted, or comes from joining in the compacted ID from another
                # pre-materialized table.
                cols: List[sa.ColumnElement] = []
                select_from: sa.FromClause = subq
                joined_tables = set()
                for col in src_table.table_obj.columns:
                    if col.name in compact_cols:
                        cols.append(id_col.label(col.name))
                    elif col.name in remote_compact_cols:
                        dst_schema, dst_table_name, dst_col = remote_compact_cols[
                            col.name
                        ]
                        dst_table = build_ctx(
                            SQLTableIdentifier(
                                table_schema=dst_schema,
                                table_name=dst_table_name,
                                sampled=True,
                            )
                        )
                        if dst_table in joined_tables:
                            dst_table = dst_table.alias()
                        else:
                            joined_tables.add(dst_table)
                        cols.append(
                            dst_table.columns[SUBSETTER_COMPACT_COLUMN].label(col.name)
                        )
                        select_from = sa.join(
                            select_from,
                            dst_table,
                            subq.columns[col.name] == dst_table.columns[dst_col],
                            isouter=True,
                        )
                    else:
                        cols.append(subq.columns[col.name])

                query_stmt = sa.select(*cols).select_from(select_from)

            LOGGER.debug(
                "  Using statement %s",
                str(query_stmt.compile(dialect=conn.engine.dialect)).replace("\n", " "),
            )
            result = conn.execution_options(
                stream_results=True,
                yield_per=SOURCE_BUFFER_SIZE,
            ).execute(query_stmt)
            columns = result.keys()

            filter_view = FilterViewChain.construct_filter(
                columns,
                self.config.filters.get(table, []),
            )

            rows = 0

            def _count_rows(result):
                nonlocal rows
                for row in tqdm(result, desc="row progress", unit="rows"):
                    # result_processor
                    rows += 1
                    yield row

            column_multipliers = table_column_multipliers.get(table)
            output.output_result_set(
                schema,
                table_name,
                columns,
                _count_rows(result),
                filter_view=filter_view,
                multiplier=(
                    1
                    if column_multipliers is None
                    else self.config.multiplicity.multiplier
                ),
                column_multipliers=column_multipliers,
            )
            LOGGER.info("Sampled %d rows for %s.%s", rows, schema, table_name)

    def _validate_filters(self, meta: DatabaseMetadata):
        for table, filters in self.config.filters.items():
            if not filters:
                continue

            schema, table_name = parse_table_name(table)
            tbl = meta.tables.get((schema, table_name))
            if tbl is None:
                LOGGER.warning("Found filters for unknown table %s", table)
                continue

            FilterViewChain.construct_filter(
                tuple(column.name for column in tbl.table_obj.columns),
                filters,
            )

    def _get_multiplied_columns(
        self, meta: DatabaseMetadata, plan: SubsetPlan
    ) -> Dict[str, Set[str]]:
        """
        Computes a mapping of tables to the list of columns that should be multiplied.
        Here a 'multiplied' column must be an integer column that should be updated as
        (column_value * multiplier + i) for each multiplied record 0 <= i < multiplier.

        Generally any column that's not part of a passthrough table that meets these
        criteria will be a 'multiplied' column:
        - Is explicitly listed in multiplicity.extra_columns
        - Is part of a primary key and table not listed in multiplicity.ignore_primary_key
        - Is part of a foreign key to a non-passthrough table

        Additionally the current implementation requires all multiplied columns to be
        integral. This method will raise a ValueError if this is not the case.
        """
        if self.config.multiplicity.multiplier <= 1:
            return {}

        result: Dict[str, Set[str]] = {}
        ignore_tables = set(plan.passthrough) | set(
            self.config.multiplicity.ignore_tables
        )

        for table_name in plan.queries:
            if table_name in ignore_tables:
                continue

            table = meta.tables[parse_table_name(table_name)]

            # Calculate set of directly multiplied columns for this table
            cols = set(self.config.multiplicity.extra_columns.get(table_name, []))
            ignored_pk_cols = set(
                self.config.multiplicity.ignore_primary_key_columns.get(table_name, [])
            )
            cols.update(set(table.primary_key) - ignored_pk_cols)

            result[table_name] = cols

        # Multiply foreing key columns that point at multplied columns. Being lazy
        # about this closure; realistically most databases only have FKs that point directly
        # at PKs so this should loop twice.
        while True:
            changes = False

            for table_name in plan.queries:
                table = meta.tables[parse_table_name(table_name)]

                cols = result.get(table_name, set())
                start_len = len(cols)
                for fk in table.foreign_keys:
                    dst_mapped = result.get(f"{fk.dst_schema}.{fk.dst_table}", set())
                    cols.update(set(fk.columns) & dst_mapped)

                if cols and table_name in ignore_tables:
                    raise ValueError(
                        "Passthrough foreign key points to multiplied column"
                    )

                changes = changes or len(cols) > start_len

            if not changes:
                break

        for table_name in plan.queries:
            table = meta.tables[parse_table_name(table_name)]
            col_map = {column.name: column for column in table.table_obj.columns}

            # Verify multiplied columns are integer
            for col_name in result.get(table_name, set()):
                col = col_map[col_name]
                if not issubclass(col.type.python_type, int):  # type: ignore
                    raise ValueError(
                        f"Primary key column {table_name}.{col_name} "
                        "must be integral when using multiplicity"
                    )

        return result
