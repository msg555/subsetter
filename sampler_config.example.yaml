# Optionally specify the source database. This can also be passed on the command
# line or through environment variables. 
source:
  host: my-database # overridden by SUBSET_SOURCE_HOST
  port: 3306 # overridden by SUBSET_SOURCE_PORT
  username: my_user # overridden by SUBSET_SOURCE_USERNAME
  password: my_s3cret # overridden by SUBSET_SOURCE_PASSWORD
  session_sqls: # Set any additional session variables; e.g.
    - SET @@session.max_statement_time=0
    - SET @@session.net_read_timeout=3600
    - SET @@session.net_write_timeout=3600
    - SET @@session.wait_timeout=28800
    - SET @@session.innodb_lock_wait_timeout=3600

# Optionally specify the source database. This can also be passed on the command
# line or through environment variables. 
output:
  mode: mysql
  host: my-dest-database # overridden by SUBSET_DESTINATION_HOST
  port: 3306 # overridden by SUBSET_DESTINATION_PORT
  username: my_dest_user # overridden by SUBSET_DESTINATION_USERNAME
  password: my_dest_s3cret  # overridden by SUBSET_DESTINATION_PASSWORD
  session_sqls: # Set any additional session variables; e.g.
    - SET @@session.max_statement_time=0
    - SET @@session.net_read_timeout=3600
    - SET @@session.net_write_timeout=3600
    - SET @@session.wait_timeout=28800
    - SET @@session.innodb_lock_wait_timeout=3600

# Alternative configuration to output JSON files within a directory
# output:
#   mode: directory
#   directory: some/output/dir/

# Filters allow you to anonymize and shape your output dataset.
filters:

  # A list of filters can be given for each table. Filters are applied in order
  # with the results of the last filter being part of the output.
  db1.users:

  # The 'null' secret will replace the sampled value with null for the given
  # columns. The 'zero' filter works the same way but instead replaces the value
  # with a 'zero' value of the same type (e.g. empty string or 0)
  - op: "null"
    columns: [big_secret]

  # The 'omit' filter will remove the given column from the output set. This can
  # be useful if that column doesn't exist in your output or it cannot be set
  # directly.
  - op: omit
    columns: [extra_column]

  # The 'constant' filter can be used to replace a column's value with a
  # constant value instead.
  - op: constant
    columns: [is_active, role]
    values: [1, admin]

  # The 'random_int' and 'random_float' filters can replace a value with a
  # random int or float in the desired range (inclusive).
  - op: random_int
    columns: [test_score]
    low: 0
    high: 100

  # The 'random_string' filter can replace column values with random textual
  # data in a variety of forms.
  - op: random_string

    # Here we generate a hex string of length 16. Other 'alphabets' include
    # "alnum", "hex_lower", "hex_upper", "digit", "alpha", "alpha_lower",
    # "alpha_upper".
    alphabet: hex
    length: 16

  # The 'random_uuid' replaces the given column with a random uuid.
  - op: random_uuid
    columns: [gizmo_uuid]

  # The remaining filters are wrappers around the corresponding Faker generator
  # (see https://faker.readthedocs.io/en/master/). The full list of supported
  # filters is:
  # - fake_email
  # - fake_first_name
  # - fake_last_name
  # - fake_name
  # - fake_phone_number
  # - fake_license_plate
  # - fake_vin
  # - fake_address
  # - fake_building_number
  # - fake_city
  # - fake_state
  # - fake_state_abbr
  # - fake_country
  # - fake_country_code
  # - fake_postal_code
  # - fake_street_address
  # - fake_street_name
  # - fake_latitude
  # - fake_longitude
  #
  # If the optional property 'unique' is set Faker will use the 'unique'
  # interface. Note that this can lead to failures if generating a lot of data
  # or setting on filters with limited output space (e.g. "fake_state")
  - op: fake_email
    columns: [email_address]
    unique: true # optional, if set uses the 'unique' faker interface

  # Arbitrary filters can be implemented dynamically. To do so create a Python
  # module that can be imported by the subsetter. The module should define a
  # factory method that will be passed a list of the requested columns and
  # should return a list of the values after filtering them. For example you
  # could write:
  #
  # class MyFilter:
  #     def __init__(self, mysuffix: str):
  #         self.suffix = mysuffix
  #
  #         def __call__(self, values):
  #             return [f"{value}-{self.suffix}" for value in values]
  - op: plugin
    columns: [col1, col2]
    module: fully.qualified.module
    class: MyFilter
    kwargs:
      mysuffix: hello!

# The sampler supports multiplying rows for generating additional data. By
# default this operates by mapping all primary keys and foreign keys into a
# new key-space.
#
# Note: This functionality is currently only limited to integral primary
# keys/foreign keys. Other types of keys, e.g. uuids, could be supported in
# the future. Non-opaque keys, like email addresses, will not be supported.
# Integer keys will be mapped into a new key-space using the equation:
#
#   new_id = source_id * multiplier + instance_id
multiplicity:
  # Multiplication factor. If set to 1 the rest of these fields are ignored.
  multiplier: 10

  # Similar to planner_config.infer_foreign_keys
  infer_foreign_keys: true

  # Tables that should not be multiplied. This often should just be the same
  # as your plan passthrough tables.
  passthrough:
    - db1.ui_features

  # Extra columns that should have their ID-space mapped that aren't part of a
  # primary key or foreign key.
  extra_columns:
    db2.gadgets: [user_id]

  # Some tables may have multiple columns that make up the primary key. By
  # default every column will be mapped; you can set this field to disable
  # mapping of some of them.
  ignore_primary_key_columns:
    db1.gizmo: [setting_name]
